# 6.特征分配与softmax机制

#### self-attetion如何计算？

![image-20200609111728632](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609111728.png)

前面通过q和k的内积得到一个”分值“，我们还需要把所有分值按照一个比例来”描述“（对于每个词，需要知道其他词的影响），可以用到softmax做一个归一化，就可以得到一个影响程度的大小矩阵。

上图右上角，对于Q矩阵和K矩阵的乘积除以一个向量维度的根号（除以这个数是因为，有的时候向量维度越大得到内积越大，这样就会得出向量维度越大->权重越大这样不太合理的结论，因此要让分值不能因为向量维度的增大而增加）做Softmax，可以得到一个归一化的实际特征矩阵V。

#### 每个词的Attention计算

 	每个词的Q会跟整个序列中每一个K计算得分（softmax后得到value），然后基于得分再分配特征

![image-20200609112442765](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609112442.png)

#### Attention整体计算流程

1. 每个词的Q会跟每一个K计算得分
2. Softmax后就得到整个加权结果
3. 此时每个词看的不只是它前面的序列而是整个输入序列（这一点区别于RNN）
4. 同一时间计算出所有词的表示结果

![image-20200609112752659](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609112752.png)

q1 * k1, q1 * k2得到内积后，除以向量维度的根号(假设向量维度为64，因此这里除以8)得到14和12，再做一个softmax得到0.88和0.12，0.88代表第一个词x1对于第一个词x1比较重要（0.88），第二个词x2对于第一个词x1比较不重要（0.12），因此第一个词最终的编码向量z1就为z1 = 0.88 * v1 + 0.12 * v2 （value表示当前这个词本身的词向量含义，z就是考虑到了文本中其他的词的影响）。

其他词同理，也可以得到最终的编码向量。

由于都是和三个辅助矩阵做矩阵乘法（而不像RNN需要文本中前面单词的中间结果），因此Transformer可以并行的计算每个词的最终编码向量。