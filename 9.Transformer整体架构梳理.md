# 9.Transformer整体架构梳理

Transformer有encoder端和decoder端。

Decoder：

把向量翻译成我们需要的样子（比如机器翻译）。

![image-20200609184151437](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609184152.png)

#### Decoder

1. Attention的计算和Encoder端不同

![image-20200609184415224](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609184415.png)

​	比如上图，左边是encoder，右边是decoder，可以发现encoder端的输出会到decoder端做decode，然而不光是encoder端的输出，在图的右下角的output部分，其实输入的是原来的向量（未经过encoder的），这个向量会在decoder端先做self-attention得到新的向量，然后将两个向量（encoder端输出的向量和decoder端做了self-attention之后的原始向量）连接之后再在decoder端做self-attention。图的右上部分两个向量连接之后做的self-attention就叫做encoder-decoder-attention。 

2. 加入了MASK机制

   ​	在encoder端，一个词和序列中其他所有词是个什么关系我们都可以马上知道（通过计算内积，self-attention等机制），但是在decoder端是不可以的。decoder端是一个一个出答案的。比如我们要做一个机器翻译，要把法语的je suis etudiant翻译成英语，这个翻译过程应该是按照顺序翻译的，而不是一下子把整句话翻译出来，所以翻译某个词的时候要考虑前面的词，但是不能考虑后面的词（因为后面的词还不知道呢）。比如我们已经翻译了I am，要翻译第三个词的时候，自然要考虑前面这两个词，但是不能考虑第四个词，因为还不知道第四个词呢，翻译第四个词的时候实际上也要考虑前面的词（包括第三个词）。所以在decoder端不同于encoder端，某个词无法立即知道和其他词的关系。

   ​	因此在decoder端引入了MASK机制，可以先把后面的词掩盖掉（MASK)，这样在decode当前词向量的时候就不会考虑到后面的词向量的影响。对每个词做编码的时候只能考虑前面已有的词，后面还没出结果的不能做利用。

![image-20200609184243756](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609184243.png)

 除了上面这两点，decoder端的self-attention计算其他部分和encoder端是一样的。

#### 最终输出结果

1. 得出最终预测结果
2. 损失函数cross-entropy即可

![image-20200609190120224](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609193401.png)

​	得出结果和其他分类任务相同。比如最终得到一个特征，特征再连一个softmax层，可以得到一个词在当前语料库中的哪一个（分类任务），如语料库有一万个词，我们需要输出当前的词是语料库中的哪一个。

​	损失函数用交叉熵（和卷积神经网络做分类任务是一样的）。

#### Transformer整体梳理

1. Self-Attention
2. Multi-Head
3. 多层堆叠，位置编码
4. 并行加速训练

![image-20200609190734650](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609193354.png)

​	首先Inputs中输入序列，序列经过预处理（分词）划分成每个词向量，其他文本处理和其他网络结果（word2vec，RNN）相同，input embedding就是做词嵌入/初始化的。不过一般不需要再做预处理了，因为BERT是一个预训练的模型，别人都已经训练好了，所以一般数据也不需要做什么初始化了，直接在别人的基础上做一个微调(fine-tune)，Positional Encoding阶段加入一些位置信息/周期性信号（周期性信号随着周期性变化，可以理解为相当于位置做变化，也是一种常见的加入词向量位置信息的方法）啥的。

​	然后在encoder端做self-attention，一共做N层(Nx)，self-attention往往做多头(multi-head)的self-attention,图中三个箭头表示做了一个三头的self-attention。

​	另外要注意随着层数的堆叠可能学习效果越来越差了，所以要加入残差连接，一条路原封不动，就是原来的数据同等映射拿过去；另一条路让数据自己去学习。哪个学得好就用哪个。这样就保证数据至少不比原来差。

​	decoder端开始时（从图中右下角的outputs部分开始）也把数据做预处理、self-attention啥的。不过要注意decoder端的self-attention相比于encoder端的self-attention多了一个MASK机制（预测的时候不能用后面的结果作为已知条件），然后要综合考虑encoder端的输出。encoder端有输出k1,,,,,,,kn, v1,,,,,,,vn给decoder端用，Q矩阵的q1,,,,,qn用decoder端自己的，有了Q, K, V矩阵，接下来就可以做self-attention，编码之后就有向量了，向量经过softmax就可以输出一个结果（概率），分类问题就完成了。

#### BERT和Transformer是什么关系？

​	BERT就是把Transformer应用到实际任务中去做训练。

#### 效果展示

![image-20200609193344783](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609193344.png)

对于不同的一句话，做过self-attention之后，可以明白it指代的是句子中的哪个词。换句话说，BERT可以帮助机器“理解上下文”。