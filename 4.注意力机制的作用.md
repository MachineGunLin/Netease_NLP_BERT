#### 4.注意力机制的作用

#### Attention是什么意思？

1. 对于输入的数据，关注点是什么？
2. 如何才能让计算机关注到这些有价值的信息？

![image-20200609104117370](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609104819.png)

​								卷积网络中对图像提取特征，可以根据热度图来看“重点”

![image-20200609104225244](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609104800.png)

​												一句话可能需要强调不同的词

#### self-attention是什么？

attention不应该通过人为加入权重来进行判断，应该由机器自己判断。

The **animal** didn't cross the street because **it** was too **tired**. 		//it指代The animal

The animal didn't cross the **street** because **it** was too **narrow**.		//it指代The street

>  self-attention的意思就是对一个词编码的时候，不只是简单考虑当前这个词，而是考虑上下文中其他词对这个词的影响。

![image-20200609104913097](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609104913.png)

对it编码的时候，考虑到其他词的“影响”，颜色越深表示权重越大，比如这句话里"animal"的权重就比较大，“street”的权重相对就比较小了。