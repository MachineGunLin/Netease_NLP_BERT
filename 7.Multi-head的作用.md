# 7.Multi-head的作用

#### Transformer的multi-head机制

1. 一组q，k，v得到了一组当前词的特征表达

   ![image-20200609132754063](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609132754.png)

   最终只能得到一种不同的表达。如z1 = 0.88 * v1 + 0.12 * v2(最终的编码就确定下来了)

   ![image-20200609134644543](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609134644.png)

2. 类似卷积神经网络中的卷积核filter  能不能提取多种特征呢？

3. 卷积中的特征图（每个特征图都表示一个不同的特征，虽然我们不一定能解释这些特征代表什么，但特征还是越多越好的）：

![image-20200609132738551](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609132740.png)

#### multi-head机制

1. 通过不同的head得到多个特征表达（一般head的个数为8个）

   ​	x原来通过一组q，k，v（q1,k1,v1)只能得到一种特征表达z1，现在我们有多组q，k，v（假设有8个head，就有q1,k1,v1, q2,v2,z2............q7,v7,z7，于是x就有八种特征表达z0, z1, .....z7)

​	2. 将所有特征拼接在一起

​	3. 可以通过再一层全连接来降维

​		拼接后的特征太多了，需要降维。类似卷积神经网络，最后通过一个全连接层进行降维。

![image-20200609135015432](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609135015.png)

 最终的特征表达z综合了多个特征（通过多种特征提取方案q, k ,v），所以我们认为这个特征更好。

### multi-head机制

之前只有一个Wq, Wk, Wv矩阵，现在有多个Wq, Wk, Wv矩阵。

![image-20200609135805042](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609142213.png)

Q,K,V都有多个，线性计算(Linear)和Scaled Dot-Product Attention都有8个head，得到8个特征表达拼接在一起再经过全连接层就形成最终的输出结果。

![image-20200609140146276](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609142223.png)

#### multi-head结果

不同的注意力结果得到的特征向量表达也不相同。

![image-20200609140426668](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609142230.png)

通过线（热度图）可以发现，每个词和自己的注意力是比较大的，和自己的附近也比较大，远一点的词注意力也会小一点。两个图表示不同的注意力机制得到的注意力大小不同。

#### Transformer堆叠多层

1. 一层肯定是不够的
2. 计算方法都是相同的

![image-20200609140648562](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609142256.png)

​	从图的下半部分可以发现，词向量x1, x2经过一层self-attention得到新的词的表达z1, z2。

z1 融入了x1, x2的信息，z2也融入了x1, x2的信息（z1可能更关注x1，z2可能更关注x2） 。图中没画出多头multi-head，但我们认为最后输出的z1, z2是经过多头之后的结果，向量z1和z2再经过全连接层Feed Forward Neural Network进行降维得到向量r1，r2。  

​	往往对于r1, r2再加入一层包含multi-head和self-attention的encoder，计算方法都是相同的。这么做是为了提高提取特征的能力，最后得到的向量也具有更加丰富的表达能力。

