# 8.位置编码与多层堆叠

#### Transformer位置信息表达

​	在self-attention中每个词都会考虑整个序列的加权，所以其出现位置并不会对结果产生什么影响（因为计算词的向量表达的时候对矩阵做运算不包含位置信息），相当于放哪都无所谓，但是这跟实际就有些不符合了（正常来说说一句话有先后顺序，有的时候词的位置信息比较重要），我们希望能对位置有额外的认识。

![image-20200609143458939](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609143459.png)

原始的输入做了词嵌入之后得到向量x, 向量x再经过一个位置信息的编码positional encoding，传统位置信息的编码用的是one-hot编码，比如0001代表第一个词，0010代表第二个词。。（用这种方式来表示词的位置信息），现在一般不用One-hot encoding，transformer中用的是余弦/正弦的周期性表达embedding with time signal，类似的现成方法很多，不作展开。

#### Add与Normalize

![image-20200609144125695](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609144125.png)

1. 归一化：![image-20200609144054634](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609144054.png)

    在Encoder端有一个LayerNorm，就是向量在经过self-attention之后就要对层（layer）进行归一化(normalization)。

   回忆一下神经网络里的batch normalization:  比如batch里有四个数据1, 2, 3, 4（分别对应上图的四列），按照batch的维度，让这些数据均值为0，标准差为1。

   layer Normalization就没有了batch的概念，对于每一个数据（比如第三号数据），不管数据有多少个特征了，让数据自己均值为0，标准差为1。

   加上layer normalization的目的：让训练速度更快、更稳定

2. 连接：基本的残差连接方式

![image-20200609151319873](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609151319.png)

​		残差连接：一个x，一条路可以做一些变换/计算，另一条路不做任何变化。然后把两条路x做一个连接。

​		为什么要这么做呢？因为有时候我们对x做一些变换，效果未必好，对x处理之后可能效果更差了，这时候加一个x就是用来“做两手准备”，这样子就可以放心的去做很多复杂变换（self-attention什么的），然后模型可以自己去比较哪种方法的loss更小（也就说明这种变换效果好）。所以残差连接就是“多一手准备”（使得训练结果至少不比原来差 ）。

