# 5.self-attention计算方法

#### self-attention如何计算？

1. 输入经过编码后得到向量
2. 想得到当前词语上下文的关系，可以当作是加权
3. 构建三个矩阵分别来查询当前词跟其他词的关系，以及特征向量的表达

![image-20200609105318953](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609105319.png)

有两个词(Thinking，Machines)，第一步先做初始化/预处理，做一个词嵌入embedding，把一个词转换为向量x1, x2。

x1需要分别知道x1,x2对上下文做了什么贡献。x2也需要知道x1，x2分别对上下文做了什么贡献。因此需要提取特征。

构建三个矩阵queries, keys, values。这三个辅助矩阵就是用来在对每个词进行编码的时候”考虑上下文“。

#### self-attention计算

1. 三个需要训练的矩阵
2. Q：query，要去查询的
3. K：key，等着被查到
4. V：value，实际的特征信息

![image-20200609105928576](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609105928.png)

x1,x2分别和Wq,Wk,Wv（初始的矩阵权重）得到三个矩阵。

![image-20200609110039350](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609110039.png)

query记录每一个词，比如x1需要知道和每个词的关系，就需要去query查询query第一个词x1对它的影响，query第二个词x2对它的影响，所以query里插入的就是文本里的每一个词。

key矩阵就是等着被查的，要查第一个词，就取出第一个词x1（然后到query矩阵去查询）。

value矩阵中存放实际的特征信息，v1是x1的特征表达，v2是x2的特征表达。最后要提取特征就来v矩阵提取。

#### self-attention计算

1. q与k的内积有多匹配
2. 输入两个向量的到一个分值
3. K：key，等着被查的
4. V：value，实际的特征信息

![image-20200609110935689](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609110935.png)

  有了x1,x2之后，（根据初始的WQ,WK,WV权重矩阵）计算出q1,q2,k1,k2,v1,v2(由于是计算出来的，所以这些向量都有实际的数值）。

我们想知道第一个词（要查询第一个词，所以找到向量q1)和每个词（从key矩阵找到k1, k2)的关系，计算内积q1 * k1, q1 * k2。

为什么要计算内积呢？ 因为线性无关的时候内积为0，”相关性“越高内积越大，所以内积可以表示”相关性“。

通过这种方法，对于每个词，我们都可以算出序列中每个词和当前这个词的相关程度。