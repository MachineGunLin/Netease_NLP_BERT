# 传统解决方案遇到的问题

#### Transformer要做什么事？

1. 基本组成依旧是机器翻译模型中常见的Seq2Seq网络
2. 输入输出都很直观（文本，不过当然需要转换成向量），其核心架构就是中间的网络设计（在BERT里就是Transformer，传统网络架构里是RNN)

![image-20200609101911135](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609101918.png)

#### 传统的RNN网络

​	计算时有什么问题？

![image-20200609102128252](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609102128.png)

一个文本序列划分为x0,x1,,,,xt输入到RNN中，x1需要用到x0的中间结果，x2需要用到x1的中间结果，......xt需要用到xt-1的中间结果，这就导致RNN无法并行计算（这也是RNN层数不能太多的原因）。



RNN不能并行计算，因此我们需要一个可以并行的网络 ——>Transformer

1. Self-Attention机制来进行并行计算，在输入和输出都相同
2. 输出结果是同时被计算出来的，现在基本已经取代RNN了（Transformer基本取代RNN了）

注意力机制：文本序列有时候需要强调某些字/词比较重要，因此我们需要网络“注意”一点比较重要的词

![image-20200609103013293](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609103013.png)

​											RNN（上图）一步一步计算序列里的每个词

![image-20200609103153946](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609103153.png)

​												Transformer(上图)并行计算整个序列

#### 传统的word2vec

1. 表示向量时有什么问题？
2. 同一个词在不同语境有不同的意思，不同语境中相同的词如何表达
3. 预训练好的向量就永久不变了

![image-20200609103352838](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609103352.png)

不同词在不同语境有不同的含义，因此需要考虑上下文。在BERT中相同的词在不同的上下文可以得到不同的向量。

#### Transformer整体架构

1. 输入如何编码（输入一个词，如何编码成向量）？
2. 输出结果是什么？
3. Attention的目的？
4. 怎样组合在一起？

![image-20200609103729939](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/20200609103730.png)

​							Transfomer整体架构（左边是编码器Encoder,右边是解码器Decoder)